{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Optim\n",
    "\n",
    "based off jeremy howards great tutorial! https://pytorch.org/tutorials/beginner/nn_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "import gzip\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "FILENAME = \"mnist.pkl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "m, n = x_train.shape\n",
    "ny = 10\n",
    "bs =64\n",
    "\n",
    "## get a mini batch\n",
    "xb = x_train[:bs]\n",
    "yb = y_train[:bs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### accuracy metric\n",
    "\n",
    "we should also use accurarcy metric to tell us how often our model is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "class Mnist_NN(nn.Module):\n",
    "    def __init__(self, n_h):\n",
    "        super().__init__()\n",
    "        self.inp = nn.Linear(784, n_h)\n",
    "        self.hid = nn.Linear(n_h, n_h)\n",
    "        self.out = nn.Linear(n_h, 10)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        ab1 = F.relu(self.inp(xb))\n",
    "        ab2 = F.relu(self.hid(ab1))\n",
    "        ab3 = F.relu(self.out(ab2))\n",
    "        return ab3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Now use Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-49-0213e0e6bd02>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-49-0213e0e6bd02>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    #\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    #\n",
    "    #\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3243, grad_fn=<NllLossBackward>) tensor(0.0625)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fit(epochs, model, opt, loss_func):\n",
    "    for epoch in range(epochs):\n",
    "        print(epoch)\n",
    "        for i in range((m - 1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('our loss:, ', loss)\n",
    "            \n",
    "            # our optimizer \n",
    "            #\n",
    "            #\n",
    "            #\n",
    "            # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "our loss:,  tensor(2.0832, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(2.0014, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.8329, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.9169, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.9153, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.8013, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.6797, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.7280, grad_fn=<NllLossBackward>)\n",
      "1\n",
      "our loss:,  tensor(1.7548, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.5507, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.5404, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.6461, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.7038, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.5790, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.5421, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.5698, grad_fn=<NllLossBackward>)\n",
      "2\n",
      "our loss:,  tensor(1.6467, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.4357, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.4565, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.5096, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.5758, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.5323, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.5098, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.5109, grad_fn=<NllLossBackward>)\n",
      "3\n",
      "our loss:,  tensor(1.5643, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.3805, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.4083, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.4465, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.5150, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.5327, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.4697, grad_fn=<NllLossBackward>)\n",
      "our loss:,  tensor(1.4655, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "fit(1, model, opt, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
